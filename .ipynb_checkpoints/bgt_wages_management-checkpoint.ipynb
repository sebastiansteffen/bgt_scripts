{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# skill_list also contains isSpecialized, isBaseline, isSoftware. Use this.\n",
    "# occ_list also contains OccFam, Employer, Sector. Use this.\n",
    "\n",
    "# FINISHED\n",
    "# incorporated predicted wages.\n",
    "# incorporated mean, variance for BGTJobId actual wages, # skills.\n",
    "# incorporated BLS OES wages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade --user numpy\n",
    "#!{sys.executable} -m pip install --upgrade --user pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, io, requests, re, csv, zipfile, xlrd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_dir = \"/mnt/hgfs/Dropbox (MIT)/research/occupational_drift\"\n",
    "home_dir = \"/home/ssteffen/projects/ssteffen_proj/bgt_occ_change\"\n",
    "data_dir = \"/nfs/pool001/ssteffen/data\"\n",
    "bgt_data_dir = os.path.join(data_dir, 'bgt', 'US')\n",
    "other_data_dir = os.path.join(data_dir, 'other')\n",
    "bgt_derived_data_dir = os.path.join(data_dir, 'bgt_derived')\n",
    "os.chdir(home_dir)\n",
    "\n",
    "\n",
    "# FLAGS\n",
    "is_debug = False\n",
    "main_vars = ['soc6'] # can include 'socX', 'naicsX', 'occ_type', 'firm', 'fips'\n",
    "groupby_vars = ['scf', 'sc', 's', 't'] + main_vars\n",
    "time_var = 'y' # Cannot be changed to monthly since we do yearly variances.\n",
    "use_predicted_wages = False #whether to import and merge on BGT predicted wages (currently only exist for 2017, 2018).\n",
    "use_job_ad_wages = False\n",
    "use_oes_wages = False \n",
    "use_deming_skills = False\n",
    "\n",
    "if is_debug:\n",
    "    NROWS = 10000\n",
    "else:\n",
    "    NROWS = None\n",
    "\n",
    "# SOC aggregation - exclude military\n",
    "soc_list = ['55']\n",
    "# NAICS2 aggregation\n",
    "naics_list = []\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Directory: /nfs/pool001/ssteffen/data/bgt_derived/soc6_y\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(bgt_derived_data_dir, '_'.join(main_vars + [time_var]))\n",
    "print(f'Output Directory: {save_dir}')\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "df_main_y_s_f_name = os.path.join(save_dir, 'df_main_y_s.csv')\n",
    "df_main_y_sc_f_name = os.path.join(save_dir, 'df_main_y_sc.csv')\n",
    "df_main_y_scf_f_name = os.path.join(save_dir, 'df_main_y_scf.csv')\n",
    "\n",
    "df_main_y_s_w_f_name = os.path.join(save_dir, 'df_main_y_s_w.csv')\n",
    "df_main_y_sc_w_f_name = os.path.join(save_dir, 'df_main_y_sc_w.csv')\n",
    "df_main_y_scf_w_f_name = os.path.join(save_dir, 'df_main_y_scf_w.csv')\n",
    "\n",
    "df_main_y_f_name = os.path.join(save_dir, 'df_main_y.csv')\n",
    "df_y_f_name = os.path.join(save_dir, 'df_y.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    '\\slp\\s', '\\slp', '\\slp$', 'lp\\.', 'l\\.p\\.', # lp We don't clean lp$ because it could be the ending of a word.\n",
    "    '\\sllp\\s', '\\sllp', 'llp$', 'llp\\.', 'l\\.l\\.p\\.', # llp\n",
    "    '\\slllp\\s', '\\slllp', 'lllp$', 'lllp\\.', 'l\\.l\\.l\\.p\\.', # lllp\n",
    "    \n",
    "    '\\sllc\\s', '\\sllc', 'llc$', 'llc\\.', 'l\\.l\\.c\\.', #llc \n",
    "    '\\slc\\s', '\\slc', '\\slc$', 'lc\\.', 'l\\.c\\.', # lc We don't clean lc$ because it could be the ending of a word.\n",
    "    '\\sltd\\s', '\\sltd', 'ltd$', 'ltd\\.', 'l\\.t\\.d\\.', # ltd\n",
    "    '\\sco\\s', 'co\\.', '\\sco$', 'c\\.o\\.', # co We don't clean co$ and \\sco because it could be the ending or beginning of a word.\n",
    "    \n",
    "    '\\scorp\\s', 'corp\\.', '\\scorp$', # corp We don't clean corp$ and \\scorp because it could be the ending or beginning of a word. \n",
    "    '\\sinc\\s' ,'inc\\.', '\\sinc$', '\\sincorporated', '\\sincorporated$', # inc We don't clean inc$ and \\sinc because it could be the ending or beginning of a word.\n",
    "    \n",
    "    '\\spllc\\s', '\\spllc', 'pllc$', 'pllc\\.', 'p\\.l\\.l\\.c\\.', #pllc\n",
    "    'p\\.c\\.', # pc We don't clean \\spc\\s, pc$ or \\spc because it could be the ending or beginning of a word.\n",
    "    \n",
    "    # Some weird state-specific ones.\n",
    "    's\\.p\\.a\\.', 'chtd\\.', '\\schtd',\n",
    "    'r\\.l\\.l\\.l\\.p\\.', 'rlllp', 'rlllp\\.', # rlllp\n",
    "    'r\\.l\\.l\\.p\\.', 'rllp', 'rllp\\.', # rllp \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _index_containing_substring(the_list, substring):\n",
    "    for i, s in enumerate(the_list):\n",
    "        if substring in s:\n",
    "              return i\n",
    "            \n",
    "def _get_bls_year_data(year):\n",
    "    '''\n",
    "    Download zipped BLS data for a given year.\n",
    "    Then unzip, clean, and return it as a dataframe.\n",
    "    Example url to the BLS zip file: https://www.bls.gov/oes/special.requests/oesm10nat.zip\n",
    "    Source: https://www.bls.gov/oes/tables.htm#2010\n",
    "    TODO: use A_MEAN since it has less missing and may account for hourly seasonality.\n",
    "    '''\n",
    "    if len(str(year)) == 4:\n",
    "        # Allow 4-digit years.\n",
    "        year = int(str(year)[-2:])\n",
    "    url = 'https://www.bls.gov/oes/special.requests/oesm{0}nat.zip'.format(year)\n",
    "    if is_debug:\n",
    "        print(url, year)\n",
    "    response = requests.get(url)\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    files = zip_file.namelist()\n",
    "    index = _index_containing_substring(files, 'national')\n",
    "    with zip_file.open(files[index], 'r') as f:\n",
    "            print(files)\n",
    "            try:\n",
    "                oes = pd.read_excel(f)\n",
    "            except:\n",
    "                oes = pd.read_excel(f)\n",
    "    if year in list(range(10, 12)):\n",
    "            oes = oes[oes['GROUP'].isnull()]\n",
    "            oes = oes.replace('*',np.nan)\n",
    "            oes = oes.replace('#',np.nan)\n",
    "            oes = oes[oes['H_MEAN'].notnull()]\n",
    "            oes = oes[oes['A_MEAN'].notnull()]\n",
    "            #oes = oes[oes['H_MEDIAN'].notnull()]\n",
    "            #oes = oes[oes['A_MEDIAN'].notnull()]\n",
    "            \n",
    "            # Calculate values to be reported as descriptive statistics\n",
    "            wageE2010 = oes[oes['H_MEAN'].notnull()]['TOT_EMP'].sum()\n",
    "            wageO2010 = oes[oes['H_MEAN'].notnull()]['TOT_EMP'].sum()\n",
    "            oes[oes['H_MEAN'].notnull()]['TOT_EMP'].count()\n",
    "            wage2010 = oes[oes['H_MEAN'].isnull()]['TOT_EMP']\n",
    "            # Add a year column\n",
    "            oes['year'] = '20{}'.format(year)\n",
    "            # Calculate employment share\n",
    "            oes['occEmpShare'] = oes['TOT_EMP'] / oes['TOT_EMP'].sum()\n",
    "            # Drop unnecessary columns\n",
    "            oes = oes[['OCC_CODE', 'OCC_TITLE', 'TOT_EMP', 'H_MEAN', 'A_MEAN', 'year', 'occEmpShare']]\n",
    "    elif year in list(range(12, 19)):\n",
    "        oes = oes[oes['OCC_GROUP']=='detailed']\n",
    "        oes = oes.replace('*',np.nan)\n",
    "        oes = oes.replace('#',np.nan)\n",
    "        oes = oes[oes['H_MEAN'].notnull()]\n",
    "        oes = oes[oes['A_MEAN'].notnull()]\n",
    "            # Calculate values to be reported as descriptive statistics                                                   \n",
    "        tot2017 = oes['TOT_EMP']\n",
    "        wage2017 = oes[oes['H_MEAN'].notnull()]['TOT_EMP']\n",
    "        # Add a year column\n",
    "        oes['year'] = '20{}'.format(year)\n",
    "        # Calculate employment share\n",
    "        oes['occEmpShare'] = oes['TOT_EMP'] / oes['TOT_EMP'].sum()\n",
    "        # Drop unnecessary columns\n",
    "        oes = oes[['OCC_CODE', 'OCC_TITLE', 'TOT_EMP', 'H_MEAN', 'A_MEAN', 'year', 'occEmpShare']]\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    oes['year'] = oes['year'].astype(np.int64)\n",
    "    oes['year_tot_emp'] = oes.groupby('year')['TOT_EMP'].transform('sum')\n",
    "    oes['emp_share'] = oes['TOT_EMP'] / oes['year_tot_emp']\n",
    "    return(oes)\n",
    "\n",
    "def _get_bls_data(years = range(10, 19)):\n",
    "    '''\n",
    "    Get all of the BLS data and append it.\n",
    "    '''\n",
    "    oes = pd.DataFrame()\n",
    "    for year in list(years):\n",
    "        if is_debug:\n",
    "            print('Importing BLS data for May 20{0}.'.format(year))  \n",
    "        oes = oes.append(_get_bls_year_data(year))\n",
    "    print(\"Finished importing BLS data.\")\n",
    "    return(oes)\n",
    "\n",
    "if is_debug:\n",
    "    oes = _get_bls_data()\n",
    "    fp = data_dir + \"/other/soc_emp.csv\"\n",
    "    print(f'Saving BLS wages, employment data to: {fp}')\n",
    "    oes.to_csv(fp, sep = '\\t', index = False)\n",
    "    # oes = pd.read_csv(fp, sep = '\\t')\n",
    "    display(oes.head(5))\n",
    "\n",
    "    \n",
    "def m2(x):\n",
    "    mean = np.mean(x)\n",
    "    count = len(x)\n",
    "    result = np.sum(np.power(np.subtract(x, [mean] * count), 2))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n",
    "def update(newValues, existingAggregate = (0, 0.0, 0.0)):\n",
    "    if isinstance(newValues, (int, float, complex)):\n",
    "        # Handle single digits.\n",
    "        newValues = [newValues]\n",
    "        \n",
    "    (count, mean, M2) = existingAggregate\n",
    "    count += len(newValues) \n",
    "    # newvalues - oldMean\n",
    "    delta = np.subtract(newValues, [mean] * len(newValues))\n",
    "    mean += np.sum(delta / count)\n",
    "    # newvalues - newMeant\n",
    "    delta2 = np.subtract(newValues, [mean] * len(newValues))\n",
    "    M2 += np.sum(delta * delta2)\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "def update1(newValue, existingAggregate = (0, 0.0, 0.0)):\n",
    "# Only for single number updates.\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    count += 1 \n",
    "    delta = newValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = newValue - mean\n",
    "    M2 += delta * delta2\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "# retrieve the mean, standard deviation and sample standard deviation\n",
    "def finalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance, sampleVariance) = (mean, M2/count, M2/(count - 1)) \n",
    "    if count == 0:\n",
    "        return float('nan')\n",
    "    elif count == 1:\n",
    "        return (mean, 0.0, 0.0)\n",
    "    else:\n",
    "        return (mean, np.sqrt(variance), np.sqrt(sampleVariance))\n",
    "    \n",
    "def lower_case_list(l):\n",
    "    return [x.lower() for x in l]   \n",
    "\n",
    "def pretty_plot_top_n(series, top_n=5, index_level=0):\n",
    "    '''\n",
    "    Source: https://sigdelta.com/blog/text-analysis-in-pandas/\n",
    "    Usage: pretty_plot_top_n(counts['n_w'])\n",
    "    Requires a (grouped) pd.Series\n",
    "    '''\n",
    "    r = series\\\n",
    "    .groupby(level=index_level)\\\n",
    "    .nlargest(top_n)\\\n",
    "    .reset_index(level=index_level, drop=True)\n",
    "    r.plot.bar()\n",
    "    return r.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certs\n",
      "CIP\n",
      "Degree\n",
      "Main\n",
      "Major\n",
      "Skill\n",
      "Sample\n",
      "Text\n"
     ]
    }
   ],
   "source": [
    "# BGT wage predictions\n",
    "# need to merge these on and see what year they are for.\n",
    "\n",
    "# BGT data\n",
    "file_list = {}\n",
    "for data_files in next(os.walk(bgt_data_dir))[1]:\n",
    "    print(data_files)\n",
    "    files = []\n",
    "    for r, d, f in os.walk(os.path.join(bgt_data_dir, data_files)):\n",
    "        for file in f:\n",
    "            if '.zip' in file:\n",
    "                files.append(os.path.join(r, file)) \n",
    "        file_list[data_files] = files\n",
    "\n",
    "main_file_list = file_list['Main']\n",
    "skill_file_list = file_list['Skill']\n",
    "# if is_debug:\n",
    "#     main_file_list = main_file_list[5:36:12] \n",
    "#     skill_file_list = skill_file_list[5:36:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tax = pd.read_csv(os.path.join(other_data_dir, 'skill_taxonomy_deming.csv'), sep='\\t')\n",
    "# y = 2010\n",
    "# m = 10\n",
    "# y_m = '{0}-{1:02d}'.format(y, m)\n",
    "# relevant_skill_file = [s for s in skill_file_list if y_m in s]\n",
    "# skill_list = pd.read_csv(relevant_skill_file[0], compression='zip', encoding='latin_1', sep='\\t', na_values = [-999, 'na'], dtype={'BGTJobId': np.int64, 'Skill': str, 'SkillCluster': str, 'SkillClusterFamily': str})\n",
    "# skill_list = skill_list.merge(right=df_tax, on='Skill', how='inner')\n",
    "# display(skill_list[skill_list['num_deming_skills']>0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique (skill, ym, soc6, naics6) counts.\n",
      "Ingesting data for 2007-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssteffen/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1,39,40,47,48,49,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data for 2007-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssteffen/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (39,40,47,48,49,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data for 2007-03.\n",
      "Ingesting data for 2007-04.\n",
      "Ingesting data for 2007-05.\n",
      "Ingesting data for 2007-06.\n",
      "Ingesting data for 2007-07.\n",
      "Ingesting data for 2007-08.\n",
      "Ingesting data for 2007-09.\n",
      "Ingesting data for 2007-10.\n",
      "Ingesting data for 2007-11.\n",
      "Ingesting data for 2007-12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssteffen/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data for 2010-01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssteffen/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data for 2010-02.\n",
      "Ingesting data for 2010-03.\n",
      "Ingesting data for 2010-04.\n",
      "Ingesting data for 2010-05.\n",
      "Ingesting data for 2010-06.\n",
      "Ingesting data for 2010-07.\n",
      "Ingesting data for 2010-08.\n",
      "Ingesting data for 2010-09.\n",
      "Ingesting data for 2010-10.\n",
      "Ingesting data for 2010-11.\n",
      "Ingesting data for 2010-12.\n",
      "Ingesting data for 2011-01.\n",
      "Ingesting data for 2011-02.\n",
      "Ingesting data for 2011-03.\n",
      "Ingesting data for 2011-04.\n",
      "Ingesting data for 2011-05.\n",
      "Ingesting data for 2011-06.\n",
      "Ingesting data for 2011-07.\n",
      "Ingesting data for 2011-08.\n",
      "Ingesting data for 2011-09.\n",
      "Ingesting data for 2011-10.\n",
      "Ingesting data for 2011-11.\n",
      "Ingesting data for 2011-12.\n",
      "Ingesting data for 2012-01.\n",
      "Ingesting data for 2012-02.\n",
      "Ingesting data for 2012-03.\n",
      "Ingesting data for 2012-04.\n",
      "Ingesting data for 2012-05.\n",
      "Ingesting data for 2012-06.\n",
      "Ingesting data for 2012-07.\n",
      "Ingesting data for 2012-08.\n",
      "Ingesting data for 2012-09.\n",
      "Ingesting data for 2012-10.\n",
      "Ingesting data for 2012-11.\n",
      "Ingesting data for 2012-12.\n",
      "Ingesting data for 2013-01.\n",
      "Ingesting data for 2013-02.\n",
      "Ingesting data for 2013-03.\n",
      "Ingesting data for 2013-04.\n",
      "Ingesting data for 2013-05.\n",
      "Ingesting data for 2013-06.\n",
      "Ingesting data for 2013-07.\n",
      "Ingesting data for 2013-08.\n",
      "Ingesting data for 2013-09.\n",
      "Ingesting data for 2013-10.\n",
      "Ingesting data for 2013-11.\n",
      "Ingesting data for 2013-12.\n",
      "Ingesting data for 2014-01.\n",
      "Ingesting data for 2014-02.\n",
      "Ingesting data for 2014-03.\n",
      "Ingesting data for 2014-04.\n",
      "Ingesting data for 2014-05.\n",
      "Ingesting data for 2014-06.\n",
      "Ingesting data for 2014-07.\n",
      "Ingesting data for 2014-08.\n",
      "Ingesting data for 2014-09.\n",
      "Ingesting data for 2014-10.\n",
      "Ingesting data for 2014-11.\n",
      "Ingesting data for 2014-12.\n",
      "Ingesting data for 2015-01.\n",
      "Ingesting data for 2015-02.\n",
      "Ingesting data for 2015-03.\n",
      "Ingesting data for 2015-04.\n",
      "Ingesting data for 2015-05.\n",
      "Ingesting data for 2015-06.\n",
      "Ingesting data for 2015-07.\n",
      "Ingesting data for 2015-08.\n",
      "Ingesting data for 2015-09.\n",
      "Ingesting data for 2015-10.\n",
      "Ingesting data for 2015-11.\n",
      "Ingesting data for 2015-12.\n",
      "Ingesting data for 2016-01.\n",
      "Ingesting data for 2016-02.\n",
      "Ingesting data for 2016-03.\n",
      "Ingesting data for 2016-04.\n",
      "Ingesting data for 2016-05.\n",
      "Ingesting data for 2016-06.\n",
      "Ingesting data for 2016-07.\n",
      "Ingesting data for 2016-08.\n",
      "Ingesting data for 2016-09.\n",
      "Ingesting data for 2016-10.\n",
      "Ingesting data for 2016-11.\n",
      "Ingesting data for 2016-12.\n",
      "Ingesting data for 2017-01.\n",
      "Ingesting data for 2017-02.\n",
      "Ingesting data for 2017-03.\n",
      "Ingesting data for 2017-04.\n",
      "Ingesting data for 2017-05.\n",
      "Ingesting data for 2017-06.\n",
      "Ingesting data for 2017-07.\n",
      "Ingesting data for 2017-08.\n",
      "Ingesting data for 2017-09.\n",
      "Ingesting data for 2017-10.\n",
      "Ingesting data for 2017-11.\n",
      "Ingesting data for 2017-12.\n",
      "Ingesting data for 2018-01.\n",
      "Ingesting data for 2018-02.\n",
      "Ingesting data for 2018-03.\n",
      "Ingesting data for 2018-04.\n",
      "Ingesting data for 2018-05.\n",
      "Ingesting data for 2018-06.\n",
      "Ingesting data for 2018-07.\n",
      "Ingesting data for 2018-08.\n",
      "Ingesting data for 2018-09.\n",
      "Ingesting data for 2018-10.\n",
      "Ingesting data for 2018-11.\n",
      "Ingesting data for 2018-12.\n",
      "Ingesting data for 2019-01.\n",
      "Ingesting data for 2019-02.\n",
      "Ingesting data for 2019-03.\n",
      "Ingesting data for 2019-04.\n",
      "Ingesting data for 2019-05.\n",
      "Ingesting data for 2019-06.\n",
      "Ingesting data for 2019-07.\n",
      "Ingesting data for 2019-08.\n",
      "Ingesting data for 2019-09.\n",
      "Ingesting data for 2019-10.\n"
     ]
    }
   ],
   "source": [
    "#%%script false\n",
    "# Skills are unique at the BGTJobId level, but skill clusters and skill cluster families are not. \n",
    "# Thus, if a BGTJobId contains many buzzword skills, we would overcount the actual number of skill clusters and skill cluster families.\n",
    "# To get around this, I create binary counts at the skill cluster and skill cluster family level separately.\n",
    "\n",
    "print(f'Unique (skill, ym, {\", \".join(str(x) for x in main_vars)}) counts.')\n",
    "# Download Skill data.\n",
    "# Loop over each month file to compile the list we want.\n",
    "\n",
    "if use_predicted_wages:\n",
    "    df_wages1 = pd.read_csv(os.path.join(other_data_dir, 'salary1_clean.csv'), encoding='latin_1', sep='\\t', na_values = [-999, 'na'])\n",
    "    df_wages2 = pd.read_csv(os.path.join(other_data_dir, 'salary-reproc-111018_clean.csv'), encoding='latin_1', sep='\\t', na_values = [-999, 'na'])\n",
    "\n",
    "if use_oes_wages:\n",
    "    oes_fp = data_dir + \"/other/soc_emp.csv\"\n",
    "    #oes = _get_bls_data()\n",
    "    oes = pd.read_csv(oes_fp, sep = '\\t')\n",
    "\n",
    "if use_deming_skills:\n",
    "    df_tax = pd.read_csv(os.path.join(other_data_dir, 'skill_taxonomy_deming.csv'), sep='\\t')\n",
    "    \n",
    "#try: del big_ct\n",
    "#except: pass\n",
    "if is_debug:\n",
    "    years = [2015, 2017]\n",
    "    months = [1, 2]\n",
    "else:\n",
    "    years = [2007] + list(range(2010, 2024))\n",
    "    months = list(range(1, 13))\n",
    "\n",
    "df_main_y_s_all = []\n",
    "df_main_y_sc_all = []\n",
    "df_main_y_scf_all = []\n",
    "\n",
    "df_main_y_s_w_all = []\n",
    "df_main_y_sc_w_all = []\n",
    "df_main_y_scf_w_all = []\n",
    "\n",
    "df_main_y_all = []\n",
    "df_y_all = []\n",
    "\n",
    "for y in years:\n",
    "    df_y = []\n",
    "    for m in months:\n",
    "        y_m = '{0}-{1:02d}'.format(y, m)\n",
    "        print('Ingesting data for {0}.'.format(y_m))\n",
    "        # Find the relevant file\n",
    "        relevant_main_file = [s for s in main_file_list if y_m in s]\n",
    "        relevant_skill_file = [s for s in skill_file_list if y_m in s]\n",
    "\n",
    "        # Import main file\n",
    "        if not relevant_main_file:\n",
    "            continue\n",
    "            \n",
    "        occ_list = pd.read_csv(relevant_main_file[0], compression='zip', nrows = NROWS, \n",
    "                               encoding='latin_1', sep='\\t', na_values = [-999, 'na'])\n",
    "        occ_list.rename(columns = {'SOC':'soc', 'SOCName':'soc_name', 'Employer':'firm',\n",
    "                                   'Sector':'sector', 'SectorName':'sector_name', 'NAICS3':'naics3', \n",
    "                                   'NAICS4':'naics4', 'NAICS5':'naics5', 'NAICS6':'naics6',\n",
    "                                  'City':'city', 'State':'state', 'County':'county', 'FIPSState':'fips_state',\n",
    "                                  'FIPSCounty':'fips_county', 'FIPS':'fips', 'Lat':'lat', 'Lon':'lon',\n",
    "                                  'BestFitMSA':'best_fit_msa', 'BestFitMSAName':'best_fit_msa_name',\n",
    "                                   'BestFitMSAType':'best_fit_msa_type', 'MSA':'msa', 'MSAName':'msa_name'\n",
    "                                  }, \n",
    "                        inplace = True)\n",
    "        \n",
    "        occ_list = occ_list[['BGTJobId', 'soc', 'soc_name', 'firm', 'sector', 'sector_name', 'naics3', 'naics4', 'naics5',\n",
    "                            'naics6', 'city', 'state', 'county', 'fips_state', 'fips_county', 'fips', 'lat',\n",
    "                            'lon', 'msa', 'msa_name']]\n",
    "        occ_list['t'] = int(y_m[:4])\n",
    "\n",
    "        if use_job_ad_wages:\n",
    "            if is_debug:\n",
    "                print('Dropping job ads with empty wages.')  \n",
    "            occ_list['wage_a'] = occ_list[['MinSalary', 'MaxSalary']].mean(axis=1, skipna=True)\n",
    "            occ_list['wage_h'] = occ_list[['MinHrlySalary', 'MaxHrlySalary']].mean(axis=1, skipna=True)\n",
    "            occ_list.dropna(subset=['BGTJobId', 'wage_a'], inplace=True)\n",
    "            occ_list = occ_list[occ_list['wage_a' != 0.0]]\n",
    "            occ_list = occ_list[occ_list['wage_h' != 0.0]]\n",
    "\n",
    "        if use_predicted_wages:\n",
    "            # TODO: Incorporate this into the aggregation. Right now they get dropped.\n",
    "            if is_debug:\n",
    "                print('Merging on predicted wages by BGTJobId from Chewning, Liu, Gaurav (KDD 2018).')\n",
    "        # Filter out job ads for which we do not have wages.\n",
    "            occ_list = occ_list.merge(right=df_wages1, on='BGTJobId', how='inner')\n",
    "            occ_list = occ_list.merge(right=df_wages2, on='BGTJobId', how='inner')\n",
    "            if occ_list.empty:\n",
    "                print(f'No wages in {y_m}.')\n",
    "                continue  \n",
    "        \n",
    "        if np.any(['soc' in word for word in main_vars]):\n",
    "        # SOC Level cleaning.\n",
    "            occ_list.dropna(subset=['soc'], inplace=True)\n",
    "            occ_list['soc_name'] = occ_list['soc_name'].astype(str)\n",
    "            occ_list['soc'] = occ_list['soc'].astype(str)\n",
    "            occ_list['soc6'] = occ_list['soc'].str.replace('-', '', regex=False)\n",
    "            occ_list['soc4'] = occ_list['soc6'].str[:4]\n",
    "            occ_list['soc2'] = occ_list['soc6'].str[:2]\n",
    "            # Filter SOC codes.\n",
    "            if soc_list:\n",
    "                occ_list = occ_list[~occ_list['soc2'].isin(soc_list)]\n",
    "                \n",
    "        if np.any(['firm' in word for word in main_vars]):\n",
    "            if is_debug:\n",
    "                print('Dropping empty firm names.')\n",
    "                occ_list['firm'] = occ_list['firm'].str.replace('[\\t\\n\\r\\f\\v]', '')\n",
    "            occ_list.dropna(subset=['firm'], inplace=True)\n",
    "            \n",
    "        if np.any(['fips' in word for word in main_vars]):\n",
    "            if is_debug:\n",
    "                print('Dropping empty FIPS.')\n",
    "            occ_list.dropna(subset=['fips'], inplace=True)\n",
    "            \n",
    "        if use_oes_wages:\n",
    "            if is_debug:\n",
    "                print('Merging on the BLS data by soc code.')\n",
    "            occ_list = occ_list.merge(right=oes, left_on=['soc', 't'], right_on=['OCC_CODE', 'year'], how='inner') \n",
    "            occ_list['wage_a_weighted'] = occ_list['A_MEAN'] * occ_list['emp_share']\n",
    "            occ_list['wage_h_weighted'] = occ_list['H_MEAN'] * occ_list['emp_share']\n",
    "\n",
    "        if np.any(['naics' in word for word in main_vars]): \n",
    "            # NAICS Level cleaning.\n",
    "            occ_list.dropna(subset=['naics3'], inplace=True)\n",
    "            occ_list['naics3'] = occ_list['naics3'].astype(np.int64)\n",
    "            occ_list['naics2'] = occ_list['naics3'].astype(str).str[:2].astype(int)\n",
    "            # Filter NAICS codes.\n",
    "            if naics_list:\n",
    "                occ_list = occ_list[~occ_list['naics2'].isin(naics_list)] \n",
    "        \n",
    "        if np.any(['occ_type' in word for word in main_vars]):\n",
    "            occ_list.dropna(subset=['soc'], inplace=True)\n",
    "            # Management measures\n",
    "            # https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column\n",
    "            conditions = [\n",
    "            (occ_list['soc'].str.startswith('11')),\n",
    "            ((occ_list['soc'].str.slice(0, 2).apply(int) >= 13) & (occ_list['soc'].str.slice(0, 2).apply(int) <= 31)),\n",
    "            ((occ_list['soc'].str.slice(0, 2).apply(int) >= 33) & (occ_list['soc'].str.slice(0, 2).apply(int) < 55) & (occ_list['soc'].str.contains('-10')))\n",
    "            ]\n",
    "            choices = ['is_top_manager', 'is_professional', 'is_middle_manager']\n",
    "            occ_list['occ_type'] = np.select(conditions, choices, default='other')\n",
    "        \n",
    "        # Import skill file\n",
    "        skill_list = pd.read_csv(relevant_skill_file[0], compression='zip', nrows = NROWS,\n",
    "                                 encoding='latin_1', sep='\\t', na_values = [-999, 'na'], dtype={'BGTJobId': np.int64, 'Skill': str, 'SkillCluster': str, 'SkillClusterFamily': str})\n",
    "        skill_list.rename(columns = {'SkillClusterFamily':'scf', 'SkillCluster':'sc', 'Skill':'s'},\n",
    "                         inplace = True)\n",
    "        skill_list = skill_list.dropna(subset=['BGTJobId', 's'])\n",
    "        # Clean the skills.\n",
    "        skill_list['s'] = skill_list['s'].str.replace('[\\t\\n\\r\\f\\v]', '')\n",
    "        skill_list['sc'] = skill_list['sc'].str.replace('[\\t\\n\\r\\f\\v]', '')\n",
    "        skill_list['scf'] = skill_list['scf'].str.replace('[\\t\\n\\r\\f\\v]', '')\n",
    "        \n",
    "        skill_list = skill_list[['BGTJobId', 's', 'sc', 'scf']]\n",
    "        \n",
    "        skill_list['s_n'] = skill_list.groupby('BGTJobId')['s'].transform('nunique')\n",
    "        skill_list['sc_n'] = skill_list.groupby('BGTJobId')['sc'].transform('nunique')\n",
    "        skill_list['scf_n'] = skill_list.groupby('BGTJobId')['scf'].transform('nunique')\n",
    "        \n",
    "        \n",
    "        # Merge together by BGTJobId\n",
    "        if is_debug:\n",
    "            print('Merging on the skills data.')\n",
    "        df = occ_list.merge(right=skill_list, on='BGTJobId', how='inner')  \n",
    "        \n",
    "        if is_debug:\n",
    "            print('Appending to the yearly file.')\n",
    "        df_y.append(df)\n",
    "    \n",
    "    df_y = pd.concat(df_y, axis=0)\n",
    "    \n",
    "\n",
    "    \n",
    "    if is_debug:\n",
    "        print('Aggregating to (main_vars, time, skill) level.')\n",
    "    # Count by main_vars X t X skills\n",
    "    df_main_y_s = df_y[['BGTJobId'] + main_vars + ['t', 's']].groupby(main_vars + ['t', 's']).count()\n",
    "    \n",
    "    # Binary Count by main_vars X t X skill cluster\n",
    "    df_main_y_sc = df_y[['BGTJobId'] + main_vars + ['t', 'sc']].groupby(['BGTJobId'] + main_vars + ['t', 'sc']).nunique()\n",
    "    df_main_y_sc = df_main_y_sc[[]].reset_index()\n",
    "    df_main_y_sc = df_main_y_sc.groupby(main_vars + ['t', 'sc']).count()\n",
    "    \n",
    "    # Binary Count by main_vars X t X skill cluster family\n",
    "    df_main_y_scf = df_y[['BGTJobId'] + main_vars + ['t', 'scf']].groupby(['BGTJobId'] + main_vars + ['t', 'scf']).nunique()\n",
    "    df_main_y_scf = df_main_y_scf[[]].reset_index()\n",
    "    df_main_y_scf = df_main_y_scf.groupby(main_vars + ['t', 'scf']).count()\n",
    "    \n",
    "    # Posting-S-Count-Weighted S Counts by main_vars X t X S\n",
    "    df_main_y_s_w = df_y[['s_n'] + main_vars + ['t', 's']].copy()\n",
    "    df_main_y_s_w['s_n'] = 1.0 / df_main_y_s_w['s_n']\n",
    "    df_main_y_s_w = df_main_y_s_w.groupby(main_vars + ['t', 's']).sum()\n",
    "    \n",
    "    # Posting-SC-Count-Weighted SC Counts by main_vars X t X SC\n",
    "    df_main_y_sc_w = df_y[['sc_n'] + main_vars + ['t', 'sc']].copy()\n",
    "    df_main_y_sc_w['sc_n'] = 1.0 / df_main_y_sc_w['sc_n']\n",
    "    df_main_y_sc_w = df_main_y_sc_w.groupby(main_vars + ['t', 'sc']).sum()\n",
    "    \n",
    "    # Posting-SCF-Count-Weighted SCF Counts by main_vars X t X SCF\n",
    "    df_main_y_scf_w = df_y[['scf_n'] + main_vars + ['t', 'scf']].copy()\n",
    "    df_main_y_scf_w['scf_n'] = 1.0 / df_main_y_scf_w['scf_n']\n",
    "    df_main_y_scf_w = df_main_y_scf_w.groupby(main_vars + ['t', 'scf']).sum()\n",
    "    \n",
    "    \n",
    "    # Skill, Wage Variables by main_vars X t\n",
    "    if is_debug:\n",
    "        print('Aggregating to (main_vars, time) level.')\n",
    "        \n",
    "    if (not use_oes_wages):\n",
    "        df_main_y = df_y.groupby(main_vars + ['t'], as_index=False).agg({'BGTJobId': 'count',\n",
    "                                              's_n': ['mean', 'std', 'median'],\n",
    "                                              'sc_n': ['mean', 'std', 'median'],\n",
    "                                              'scf_n': ['mean', 'std', 'median']\n",
    "                                             })\n",
    "    else:\n",
    "        df_main_y = df_y.groupby(main_vars + ['t'], as_index=False).agg({'BGTJobId': 'count',\n",
    "                                              'wage_a': ['mean', 'std'],\n",
    "                                              'wage_h': ['mean', 'std'],\n",
    "                                              'wage_a_weighted': ['mean'],\n",
    "                                              'wage_h_weighted': ['mean'],                   \n",
    "                                              's_n': ['mean', 'std', 'median'],\n",
    "                                              'sc_n': ['mean', 'std', 'median'],\n",
    "                                              'scf_n': ['mean', 'std', 'median']\n",
    "                                             })      \n",
    "    \n",
    "    \n",
    "    # Rename columns.\n",
    "    df_main_y.columns = [\"_\".join(x) for x in df_main_y.columns.ravel()]\n",
    "    df_main_y.columns = [re.sub('_$', '', x) for x in df_main_y.columns]\n",
    "    df_main_y.set_index(main_vars + ['t'], inplace=True)\n",
    "    \n",
    "    # Overall Variables by t\n",
    "    if is_debug:\n",
    "        print('Aggregating to (time) level.')\n",
    "    if (not use_oes_wages):\n",
    "        df_y1 = df_y.groupby(['t'], as_index=False).agg({'BGTJobId': 'count',\n",
    "                                         's_n': ['mean', 'std', 'median'],\n",
    "                                         'sc_n': ['mean', 'std', 'median'],\n",
    "                                         'scf_n': ['mean', 'std', 'median']\n",
    "                                         })\n",
    "    else:\n",
    "        df_y1 = df_y.groupby(['t'], as_index=False).agg({'BGTJobId': 'count',\n",
    "                                         'wage_a': ['mean', 'std'],\n",
    "                                         'wage_h': ['mean', 'std'],\n",
    "                                         'wage_a_weighted': ['mean', 'std'],\n",
    "                                         'wage_h_weighted': ['mean', 'std'], \n",
    "                                         's_n': ['mean', 'std', 'median'],\n",
    "                                         'sc_n': ['mean', 'std', 'median'],\n",
    "                                         'scf_n': ['mean', 'std', 'median']\n",
    "                                         })\n",
    "    \n",
    "    # Rename columns.\n",
    "    df_y1.columns = [\"_\".join(x) for x in df_y1.columns.ravel()]\n",
    "    df_y1.columns = [re.sub('_$', '', x) for x in df_y1.columns]\n",
    "    df_y1.set_index(['t'], inplace=True)\n",
    "    \n",
    "    if is_debug:\n",
    "        print('Appending to the overall file.')\n",
    "    df_main_y_s_all.append(df_main_y_s)\n",
    "    df_main_y_sc_all.append(df_main_y_sc)\n",
    "    df_main_y_scf_all.append(df_main_y_scf)\n",
    "    \n",
    "    df_main_y_s_w_all.append(df_main_y_s_w)\n",
    "    df_main_y_sc_w_all.append(df_main_y_sc_w)\n",
    "    df_main_y_scf_w_all.append(df_main_y_scf_w)\n",
    "    \n",
    "    df_main_y_all.append(df_main_y)\n",
    "    df_y_all.append(df_y1)\n",
    "\n",
    "df_main_y_s_all = pd.concat(df_main_y_s_all, axis=0)\n",
    "df_main_y_sc_all = pd.concat(df_main_y_sc_all, axis=0)\n",
    "df_main_y_scf_all = pd.concat(df_main_y_scf_all, axis=0)\n",
    "\n",
    "df_main_y_s_w_all = pd.concat(df_main_y_s_w_all, axis=0)\n",
    "df_main_y_sc_w_all = pd.concat(df_main_y_sc_w_all, axis=0)\n",
    "df_main_y_scf_w_all = pd.concat(df_main_y_scf_w_all, axis=0)\n",
    "\n",
    "\n",
    "df_main_y_all = pd.concat(df_main_y_all, axis=0)\n",
    "df_y_all = pd.concat(df_y_all, axis=0)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all\n",
    "df_main_y_all.to_csv(df_main_y_f_name, sep='\\t')\n",
    "df_y_all.to_csv(df_y_f_name, sep='\\t')\n",
    "\n",
    "df_main_y_scf_all.to_csv(df_main_y_scf_f_name, sep='\\t')\n",
    "df_main_y_scf_w_all.to_csv(df_main_y_scf_w_f_name, sep='\\t')\n",
    "\n",
    "df_main_y_sc_all.to_csv(df_main_y_sc_f_name, sep='\\t')\n",
    "df_main_y_sc_w_all.to_csv(df_main_y_sc_w_f_name, sep='\\t')\n",
    "\n",
    "df_main_y_s_all.to_csv(df_main_y_s_f_name, sep='\\t')\n",
    "df_main_y_s_w_all.to_csv(df_main_y_s_w_f_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# df_main_y_s_all = pd.read_csv(df_main_y_s_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "# df_main_y_sc_all = pd.read_csv(df_main_y_sc_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "# df_main_y_scf_all = pd.read_csv(df_main_y_scf_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "\n",
    "# df_main_y_s_w_all = pd.read_csv(df_main_y_s_w_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "# df_main_y_sc_w_all = pd.read_csv(df_main_y_sc_w_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "# df_main_y_scf_w_all = pd.read_csv(df_main_y_scf_w_f_name, sep='\\t', index_col=[0, 1, 2])\n",
    "\n",
    "\n",
    "# df_main_y_all = pd.read_csv(df_main_y_f_name, sep='\\t', index_col=[0, 1])\n",
    "# df_y_all = pd.read_csv(df_y_f_name, sep='\\t', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eof"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
